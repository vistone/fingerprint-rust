#!/usr/bin/env python3
"""
æ–‡æ¡£åˆ†æå’Œå»é‡å·¥å…·
ç”¨äºåˆ†æé¡¹ç›®ä¸­çš„é‡å¤æ–‡æ¡£å¹¶æä¾›åˆå¹¶å»ºè®®
"""

import os
import re
from collections import defaultdict
from pathlib import Path
from typing import Dict, List, Set, Tuple

def find_markdown_files(root_dir: str = ".") -> List[Path]:
    """æŸ¥æ‰¾æ‰€æœ‰Markdownæ–‡ä»¶"""
    md_files = []
    exclude_dirs = {'.git', 'target', 'venv', 'vendor'}
    
    for root, dirs, files in os.walk(root_dir):
        # è·³è¿‡æ’é™¤çš„ç›®å½•
        dirs[:] = [d for d in dirs if d not in exclude_dirs]
        
        for file in files:
            if file.endswith('.md'):
                md_files.append(Path(root) / file)
    
    return md_files

def extract_keywords(content: str) -> Set[str]:
    """æå–æ–‡æ¡£å…³é”®è¯"""
    # ç§»é™¤ä»£ç å—å’Œé“¾æ¥
    content = re.sub(r'```.*?```', '', content, flags=re.DOTALL)
    content = re.sub(r'\[.*?\]\(.*?\)', '', content)
    
    # æå–é‡è¦è¯æ±‡
    words = re.findall(r'\b(?:Phase|API|Gateway|Rust|Python|TLS|HTTP|fingerprint|browser)\b', content, re.IGNORECASE)
    return set(words)

def calculate_similarity(doc1_content: str, doc2_content: str) -> float:
    """è®¡ç®—ä¸¤æ–‡æ¡£çš„ç›¸ä¼¼åº¦"""
    keywords1 = extract_keywords(doc1_content)
    keywords2 = extract_keywords(doc2_content)
    
    if not keywords1 and not keywords2:
        return 0.0
    
    intersection = len(keywords1.intersection(keywords2))
    union = len(keywords1.union(keywords2))
    
    return intersection / union if union > 0 else 0.0

def analyze_document_groups(md_files: List[Path]) -> Dict[str, List[Tuple[Path, float]]]:
    """åˆ†ææ–‡æ¡£åˆ†ç»„"""
    # æŒ‰ä¸»é¢˜åˆ†ç»„
    groups = defaultdict(list)
    
    phase_pattern = re.compile(r'Phase\s*[0-9.]+', re.IGNORECASE)
    api_pattern = re.compile(r'(?:API|Gateway)', re.IGNORECASE)
    architecture_pattern = re.compile(r'(?:Architecture|Design)', re.IGNORECASE)
    
    for file_path in md_files:
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
        except Exception as e:
            print(f"è­¦å‘Š: æ— æ³•è¯»å– {file_path}: {e}")
            continue
        
        # ç¡®å®šæ–‡æ¡£ä¸»é¢˜
        content_lower = content.lower()
        
        if phase_pattern.search(content):
            phase_match = phase_pattern.search(content)
            phase_num = phase_match.group().replace(' ', '')
            groups[f"Phase_{phase_num}"].append((file_path, len(content)))
        elif api_pattern.search(content) and 'gateway' in content_lower:
            groups["API_Gateway"].append((file_path, len(content)))
        elif architecture_pattern.search(content):
            groups["Architecture"].append((file_path, len(content)))
        elif 'readme' in file_path.name.lower():
            groups["README"].append((file_path, len(content)))
        else:
            # æŒ‰æ–‡ä»¶ååˆ†ç»„
            name_parts = file_path.stem.split('_')
            if len(name_parts) > 1:
                group_key = name_parts[0].capitalize()
                groups[group_key].append((file_path, len(content)))
            else:
                groups["Other"].append((file_path, len(content)))
    
    return groups

def find_similar_documents(md_files: List[Path], threshold: float = 0.7) -> List[Tuple[str, List[Path]]]:
    """æŸ¥æ‰¾ç›¸ä¼¼æ–‡æ¡£"""
    similar_groups = []
    
    # è¯»å–æ‰€æœ‰æ–‡æ¡£å†…å®¹
    doc_contents = {}
    for file_path in md_files:
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                doc_contents[file_path] = f.read()
        except Exception:
            continue
    
    # æ¯”è¾ƒæ–‡æ¡£ç›¸ä¼¼åº¦
    processed = set()
    
    for i, file1 in enumerate(md_files):
        if file1 in processed:
            continue
            
        similar_group = [file1]
        processed.add(file1)
        
        for file2 in md_files[i+1:]:
            if file2 in processed:
                continue
                
            if file1 in doc_contents and file2 in doc_contents:
                similarity = calculate_similarity(doc_contents[file1], doc_contents[file2])
                
                if similarity >= threshold:
                    similar_group.append(file2)
                    processed.add(file2)
        
        if len(similar_group) > 1:
            group_name = f"Similar_Group_{len(similar_groups) + 1}"
            similar_groups.append((group_name, similar_group))
    
    return similar_groups

def generate_report(groups: Dict[str, List[Tuple[Path, float]]], 
                   similar_docs: List[Tuple[str, List[Path]]]):
    """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
    
    print("=" * 80)
    print("ğŸ“Š é¡¹ç›®æ–‡æ¡£åˆ†ææŠ¥å‘Š")
    print("=" * 80)
    print()
    
    # æŒ‰ç»„ç»Ÿè®¡
    print("ğŸ“‚ æ–‡æ¡£åˆ†ç»„ç»Ÿè®¡:")
    print("-" * 40)
    total_docs = 0
    
    for group_name, files in sorted(groups.items()):
        count = len(files)
        total_docs += count
        avg_size = sum(size for _, size in files) // count if count > 0 else 0
        
        print(f"{group_name:20} | {count:3} ä¸ªæ–‡ä»¶ | å¹³å‡å¤§å°: {avg_size:,} å­—ç¬¦")
        
        # æ˜¾ç¤ºå‰3ä¸ªæ–‡ä»¶
        for file_path, size in sorted(files, key=lambda x: x[1], reverse=True)[:3]:
            rel_path = file_path.relative_to('.')
            print(f"  - {rel_path} ({size:,} å­—ç¬¦)")
        print()
    
    print(f"æ€»è®¡: {total_docs} ä¸ªæ–‡æ¡£æ–‡ä»¶")
    print()
    
    # ç›¸ä¼¼æ–‡æ¡£åˆ†æ
    if similar_docs:
        print("ğŸ”„ å‘ç°ç›¸ä¼¼æ–‡æ¡£ç»„:")
        print("-" * 40)
        
        for group_name, files in similar_docs:
            print(f"\n{group_name}:")
            for file_path in files:
                rel_path = file_path.relative_to('.')
                print(f"  â€¢ {rel_path}")
            
            # æ˜¾ç¤ºåˆå¹¶å»ºè®®
            print("  å»ºè®®æ“ä½œ:")
            print("  1. ä¿ç•™å†…å®¹æœ€å®Œæ•´çš„ä¸€ä¸ª")
            print("  2. å°†å…¶ä»–æ–‡æ¡£çš„é‡è¦ä¿¡æ¯æ•´åˆè¿›å»")
            print("  3. åœ¨ä¿ç•™æ–‡æ¡£ä¸­æ·»åŠ æŒ‡å‘å·²åˆ é™¤æ–‡æ¡£çš„å¼•ç”¨")
    else:
        print("âœ… æœªå‘ç°é«˜åº¦ç›¸ä¼¼çš„æ–‡æ¡£")
    
    print()
    print("=" * 80)

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” å¼€å§‹åˆ†æé¡¹ç›®æ–‡æ¡£...")
    
    # æŸ¥æ‰¾æ‰€æœ‰Markdownæ–‡ä»¶
    md_files = find_markdown_files()
    print(f"æ‰¾åˆ° {len(md_files)} ä¸ªMarkdownæ–‡ä»¶")
    
    # åˆ†ææ–‡æ¡£åˆ†ç»„
    groups = analyze_document_groups(md_files)
    
    # æŸ¥æ‰¾ç›¸ä¼¼æ–‡æ¡£
    similar_docs = find_similar_documents(md_files, threshold=0.6)
    
    # ç”ŸæˆæŠ¥å‘Š
    generate_report(groups, similar_docs)
    
    # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
    report_file = "DOCUMENT_ANALYSIS_REPORT.md"
    print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")

if __name__ == "__main__":
    main()