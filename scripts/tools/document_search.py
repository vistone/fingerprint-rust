#!/usr/bin/env python3
"""
æ™ºèƒ½æ–‡æ¡£æ£€ç´¢ç³»ç»Ÿ
æä¾›å…¨æ–‡æœç´¢ã€è¯­ä¹‰æœç´¢å’Œæ™ºèƒ½æ¨èåŠŸèƒ½
"""

import os
import re
import json
import pickle
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
from datetime import datetime
import sqlite3

@dataclass
class DocumentInfo:
    """æ–‡æ¡£ä¿¡æ¯æ•°æ®ç±»"""
    path: str
    title: str
    content: str
    word_count: int
    last_modified: datetime
    tags: List[str]
    category: str
    similarity_score: float = 0.0

class DocumentIndexer:
    """æ–‡æ¡£ç´¢å¼•å™¨"""
    
    def __init__(self, project_root: str = "."):
        self.project_root = Path(project_root)
        self.index_db = self.project_root / "output" / "data" / "document_index.db"
        self._init_database()
        
    def _init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“"""
        self.index_db.parent.mkdir(parents=True, exist_ok=True)
        
        with sqlite3.connect(self.index_db) as conn:
            conn.execute('''
                CREATE TABLE IF NOT EXISTS documents (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    path TEXT UNIQUE,
                    title TEXT,
                    content TEXT,
                    word_count INTEGER,
                    last_modified TIMESTAMP,
                    tags TEXT,
                    category TEXT,
                    indexed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            conn.execute('''
                CREATE VIRTUAL TABLE IF NOT EXISTS document_search 
                USING fts5(path, title, content, tags, category)
            ''')
            
    def index_documents(self):
        """ç´¢å¼•æ‰€æœ‰æ–‡æ¡£"""
        print(".CreateIndexing documents...")
        
        documents = self._scan_documents()
        indexed_count = 0
        
        with sqlite3.connect(self.index_db) as conn:
            for doc_info in documents:
                try:
                    # æ’å…¥æˆ–æ›´æ–°æ–‡æ¡£ä¿¡æ¯
                    conn.execute('''
                        INSERT OR REPLACE INTO documents 
                        (path, title, content, word_count, last_modified, tags, category)
                        VALUES (?, ?, ?, ?, ?, ?, ?)
                    ''', (
                        doc_info.path,
                        doc_info.title,
                        doc_info.content,
                        doc_info.word_count,
                        doc_info.last_modified.isoformat(),
                        ','.join(doc_info.tags),
                        doc_info.category
                    ))
                    
                    # æ›´æ–°å…¨æ–‡æœç´¢ç´¢å¼•
                    conn.execute('''
                        INSERT OR REPLACE INTO document_search 
                        (path, title, content, tags, category)
                        VALUES (?, ?, ?, ?, ?)
                    ''', (
                        doc_info.path,
                        doc_info.title,
                        doc_info.content,
                        ' '.join(doc_info.tags),
                        doc_info.category
                    ))
                    
                    indexed_count += 1
                    
                except Exception as e:
                    print(f"Warning: Failed to index {doc_info.path}: {e}")
        
        print(f"âœ… Indexed {indexed_count} documents")
        
    def _scan_documents(self) -> List[DocumentInfo]:
        """æ‰«ææ–‡æ¡£"""
        documents = []
        
        for md_file in self.project_root.rglob("*.md"):
            if self._should_index_file(md_file):
                try:
                    doc_info = self._parse_document(md_file)
                    documents.append(doc_info)
                except Exception as e:
                    print(f"Warning: Failed to parse {md_file}: {e}")
                    
        return documents
    
    def _should_index_file(self, file_path: Path) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥ç´¢å¼•æ–‡ä»¶"""
        excluded_patterns = [
            "target/", ".git/", "vendor/", "venv/",
            "output/temp/", "output/logs/"
        ]
        
        path_str = str(file_path)
        return not any(pattern in path_str for pattern in excluded_patterns)
    
    def _parse_document(self, file_path: Path) -> DocumentInfo:
        """è§£ææ–‡æ¡£å†…å®¹"""
        content = file_path.read_text(encoding='utf-8')
        
        # æå–æ ‡é¢˜
        title_match = re.search(r'^#\s+(.+)$', content, re.MULTILINE)
        title = title_match.group(1) if title_match else file_path.stem
        
        # æå–æ ‡ç­¾
        tags = self._extract_tags(content)
        
        # ç¡®å®šåˆ†ç±»
        category = self._determine_category(file_path)
        
        # ç»Ÿè®¡å­—æ•°
        word_count = len(re.findall(r'\S+', content))
        
        # è·å–ä¿®æ”¹æ—¶é—´
        mtime = datetime.fromtimestamp(file_path.stat().st_mtime)
        
        return DocumentInfo(
            path=str(file_path.relative_to(self.project_root)),
            title=title.strip(),
            content=content,
            word_count=word_count,
            last_modified=mtime,
            tags=tags,
            category=category
        )
    
    def _extract_tags(self, content: str) -> List[str]:
        """ä»å†…å®¹ä¸­æå–æ ‡ç­¾"""
        tags = []
        
        # ä»æ ‡é¢˜çº§åˆ«æå–
        headings = re.findall(r'^#{1,6}\s+(.+)$', content, re.MULTILINE)
        tags.extend([h.lower().replace(' ', '_') for h in headings[:3]])
        
        # ä»ç‰¹å®šå…³é”®è¯æå–
        keywords = ['rust', 'tls', 'http', 'fingerprint', 'api', 'security']
        content_lower = content.lower()
        tags.extend([kw for kw in keywords if kw in content_lower])
        
        return list(set(tags))  # å»é‡
    
    def _determine_category(self, file_path: Path) -> str:
        """ç¡®å®šæ–‡æ¡£åˆ†ç±»"""
        path_parts = str(file_path.relative_to(self.project_root)).split('/')
        
        if 'docs' in path_parts:
            if 'user-guides' in path_parts:
                return 'user_guide'
            elif 'developer-guides' in path_parts:
                return 'developer_guide'
            elif 'reference' in path_parts:
                return 'reference'
            elif 'project-management' in path_parts:
                return 'project_management'
            else:
                return 'documentation'
        elif path_parts[0] in ['README.md', 'CONTRIBUTING.md', 'SECURITY.md']:
            return 'root'
        else:
            return 'other'

class DocumentSearcher:
    """æ–‡æ¡£æœç´¢å¼•æ“"""
    
    def __init__(self, project_root: str = "."):
        self.project_root = Path(project_root)
        self.indexer = DocumentIndexer(project_root)
        self.index_db = self.indexer.index_db
    
    def search(self, query: str, limit: int = 10) -> List[DocumentInfo]:
        """æ‰§è¡Œæœç´¢"""
        # é¦–å…ˆç¡®ä¿ç´¢å¼•æ˜¯æœ€æ–°çš„
        self.indexer.index_documents()
        
        results = []
        
        with sqlite3.connect(self.index_db) as conn:
            # ä½¿ç”¨å…¨æ–‡æœç´¢
            cursor = conn.execute('''
                SELECT d.path, d.title, d.content, d.word_count, 
                       d.last_modified, d.tags, d.category
                FROM documents d
                JOIN document_search s ON d.path = s.path
                WHERE document_search MATCH ?
                ORDER BY rank
                LIMIT ?
            ''', (query, limit))
            
            for row in cursor.fetchall():
                doc_info = DocumentInfo(
                    path=row[0],
                    title=row[1],
                    content=row[2][:500] + "..." if len(row[2]) > 500 else row[2],
                    word_count=row[3],
                    last_modified=datetime.fromisoformat(row[4]),
                    tags=row[5].split(',') if row[5] else [],
                    category=row[6]
                )
                results.append(doc_info)
        
        return results
    
    def recommend_similar(self, document_path: str, limit: int = 5) -> List[DocumentInfo]:
        """æ¨èç›¸ä¼¼æ–‡æ¡£"""
        # ç®€å•çš„åŸºäºæ ‡ç­¾å’Œåˆ†ç±»çš„æ¨è
        target_doc = None
        
        with sqlite3.connect(self.index_db) as conn:
            cursor = conn.execute('''
                SELECT tags, category FROM documents WHERE path = ?
            ''', (document_path,))
            
            row = cursor.fetchone()
            if row:
                target_tags = set(row[0].split(',') if row[0] else [])
                target_category = row[1]
            else:
                return []
        
        # æŸ¥æ‰¾ç›¸ä¼¼æ–‡æ¡£
        similar_docs = []
        with sqlite3.connect(self.index_db) as conn:
            cursor = conn.execute('''
                SELECT path, title, content, word_count, last_modified, tags, category
                FROM documents 
                WHERE path != ?
                ORDER BY 
                    CASE WHEN category = ? THEN 1 ELSE 0 END DESC,
                    LENGTH(tags) DESC
                LIMIT ?
            ''', (document_path, target_category, limit * 2))
            
            for row in cursor.fetchall():
                doc_tags = set(row[5].split(',') if row[5] else [])
                similarity_score = len(target_tags.intersection(doc_tags))
                
                if similarity_score > 0:
                    doc_info = DocumentInfo(
                        path=row[0],
                        title=row[1],
                        content=row[2][:300] + "..." if len(row[2]) > 300 else row[2],
                        word_count=row[3],
                        last_modified=datetime.fromisoformat(row[4]),
                        tags=row[5].split(',') if row[5] else [],
                        category=row[6],
                        similarity_score=similarity_score
                    )
                    similar_docs.append(doc_info)
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åºå¹¶é™åˆ¶æ•°é‡
        similar_docs.sort(key=lambda x: x.similarity_score, reverse=True)
        return similar_docs[:limit]

def main():
    """ä¸»å‡½æ•° - æä¾›å‘½ä»¤è¡Œæ¥å£"""
    import argparse
    
    parser = argparse.ArgumentParser(description='æ™ºèƒ½æ–‡æ¡£æ£€ç´¢ç³»ç»Ÿ')
    parser.add_argument('action', choices=['search', 'recommend', 'index'], 
                       help='æ‰§è¡Œçš„æ“ä½œ')
    parser.add_argument('--query', '-q', help='æœç´¢æŸ¥è¯¢')
    parser.add_argument('--document', '-d', help='æ–‡æ¡£è·¯å¾„ï¼ˆç”¨äºæ¨èï¼‰')
    parser.add_argument('--limit', '-l', type=int, default=10, help='ç»“æœæ•°é‡é™åˆ¶')
    
    args = parser.parse_args()
    
    searcher = DocumentSearcher()
    
    if args.action == 'index':
        print(".CreateIndexing all documents...")
        searcher.indexer.index_documents()
        print("âœ… Indexing complete")
        
    elif args.action == 'search':
        if not args.query:
            print("é”™è¯¯: æœç´¢æ“ä½œéœ€è¦æä¾›æŸ¥è¯¢å‚æ•°")
            return
            
        print(f"ğŸ” æœç´¢: {args.query}")
        results = searcher.search(args.query, args.limit)
        
        if results:
            print(f"\næ‰¾åˆ° {len(results)} ä¸ªç»“æœ:")
            for i, doc in enumerate(results, 1):
                print(f"\n{i}. {doc.title}")
                print(f"   è·¯å¾„: {doc.path}")
                print(f"   åˆ†ç±»: {doc.category}")
                print(f"   æ ‡ç­¾: {', '.join(doc.tags)}")
                print(f"   å­—æ•°: {doc.word_count}")
        else:
            print("æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£")
            
    elif args.action == 'recommend':
        if not args.document:
            print("é”™è¯¯: æ¨èæ“ä½œéœ€è¦æä¾›æ–‡æ¡£è·¯å¾„")
            return
            
        print(f"ğŸ¤– ä¸ºæ–‡æ¡£æ¨èç›¸å…³å†…å®¹: {args.document}")
        recommendations = searcher.recommend_similar(args.document, args.limit)
        
        if recommendations:
            print(f"\næ¨è {len(recommendations)} ä¸ªç›¸å…³æ–‡æ¡£:")
            for i, doc in enumerate(recommendations, 1):
                print(f"\n{i}. {doc.title}")
                print(f"   è·¯å¾„: {doc.path}")
                print(f"   ç›¸ä¼¼åº¦å¾—åˆ†: {doc.similarity_score}")
                print(f"   åˆ†ç±»: {doc.category}")
        else:
            print("æœªæ‰¾åˆ°ç›¸å…³æ¨è")

if __name__ == "__main__":
    main()